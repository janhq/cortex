---
title: Adding a Third-Party Engine to Cortex
description: Cortex supports Engine Extensions to integrate both :ocal inference engines, and Remote APIs.
---

:::warning
ðŸš§ Cortex.cpp is currently under development. Our documentation outlines the intended behavior of Cortex, which may not yet be fully implemented in the codebase.
:::

# Guide to Adding a Third-Party Engine to Cortex

## Introduction

This guide outlines the steps to integrate a custom engine with Cortex. We hope this helps developers understand the integration process.

## Implementation Steps

### 1. Implement the Engine Interface

First, create an engine that implements the `EngineI.h` interface. Here's the interface definition:

```cpp
class EngineI {
 public:
  struct EngineLoadOption{};
  struct EngineUnloadOption{};

  virtual ~EngineI() {}

  virtual void Load(EngineLoadOption opts) = 0;
  virtual void Unload(EngineUnloadOption opts) = 0;

  // Cortex.llamacpp interface methods
  virtual void HandleChatCompletion(
      std::shared_ptr<Json::Value> json_body,
      std::function<void(Json::Value&&, Json::Value&&)>&& callback) = 0;

  virtual void HandleEmbedding(
      std::shared_ptr<Json::Value> json_body,
      std::function<void(Json::Value&&, Json::Value&&)>&& callback) = 0;

  virtual void LoadModel(
      std::shared_ptr<Json::Value> json_body,
      std::function<void(Json::Value&&, Json::Value&&)>&& callback) = 0;

  virtual void UnloadModel(
      std::shared_ptr<Json::Value> json_body,
      std::function<void(Json::Value&&, Json::Value&&)>&& callback) = 0;

  virtual void GetModelStatus(
      std::shared_ptr<Json::Value> json_body,
      std::function<void(Json::Value&&, Json::Value&&)>&& callback) = 0;

  // Compatibility and model management
  virtual bool IsSupported(const std::string& f) = 0;

  virtual void GetModels(
      std::shared_ptr<Json::Value> jsonBody,
      std::function<void(Json::Value&&, Json::Value&&)>&& callback) = 0;

  // Logging configuration
  virtual bool SetFileLogger(int max_log_lines,
                           const std::string& log_path) = 0;
  virtual void SetLogLevel(trantor::Logger::LogLevel logLevel) = 0;
};
```

Note that Cortex will call `Load` before loading any models and `Unload` when stopping the engine.

### 2. Create a Dynamic Library

We recommend using the [dylib library](https://github.com/martin-olivier/dylib) to build your dynamic library. This library provides helpful tools for creating cross-platform dynamic libraries.

### 3. Package Dependencies

Please ensure all dependencies are included with your dynamic library. This allows us to create a single, self-contained package for distribution.

### 4. Publication and Integration

#### 4.1 Publishing Your Engine (Optional)

If you wish to make your engine publicly available, you can publish it through GitHub. For reference, examine the [cortex.llamacpp releases](https://github.com/janhq/cortex.llamacpp/releases) structure:

- Each release tag should represent your version
- Include all variants within the same release
- Cortex will automatically select the most suitable variant or allow users to specify their preferred variant

#### 4.2 Integration with Cortex

Once your engine is ready, we encourage you to:

1. Notify the Cortex team about your engine for potential inclusion in our default supported engines list
2. Allow us to help test and validate your implementation

### 5. Local Testing Guide

To test your engine locally:

1. Create a directory structure following this hierarchy:

```
engines/
â””â”€â”€ cortex.llamacpp/
    â””â”€â”€ mac-arm64/
        â””â”€â”€ v0.1.40/
            â”œâ”€â”€ libengine.dylib
            â””â”€â”€ version.txt
```

2. Configure your engine:

   - Edit the `~/.cortexrc` file to register your engine name
   - Add your model with the appropriate engine field in `model.yaml`

3. Testing:
   - Start the engine
   - Load your model
   - Verify functionality

## Future Development

We're currently working on expanding support for additional release sources to make distribution more flexible.

## Contributing

We welcome suggestions and contributions to improve this integration process. Please feel free to submit issues or pull requests through our repository.
